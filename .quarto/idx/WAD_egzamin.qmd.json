{"title":"Wielowymiarowa analiza danych","markdown":{"headingText":"Wielowymiarowa Analiza Danych","headingAttr":{"id":"","classes":["unnumbered"],"keyvalue":[]},"containsRefs":false,"markdown":"\n---\ntitle: Wielowymiarowa analiza danych \nauthor: Michał Koziński\nformat: \n  html:\n    page-layout: full\n    toc: true\n    toc-depth: 3\n    page-layout: full\n    embed-resources: true\n    self-contained: true\ndate: \"02-10-2023\"\n---\n\n```{r config, echo=FALSE, include=FALSE, eval=TRUE, warning=FALSE, message=FALSE}\nlibrary(magick)\n# setwd(\"C:/Users/Michc/Dropbox/Uczelnia/Semestr 5/Wielowymiarowa analiza danych/egzamin\")\n```\n\n```{r printer, eval=FALSE, echo=FALSE, message=FALSE, include=FALSE} \n# img1 <- image_transparent(image_read(\"obrazki/image5.png\"), color = \"white\")\n# image1 <- image_negate(img1)\n\n\n\nimage1 <- image_negate(image_read(\"obrazki/image7T.png\"))\n\nimage_write(image1, path = \"obrazki/Timage7.png\", format = \"png\")\n```\n\n\n# Zagadnienia do przygotowania na egzamin ustny z Wielowymiarowej Analizy Danych\n\n___\n\n<br/>\n\n## 1. Czym się różni test jednowymiarowy od testu wielowymiarowego?\n\nUżycie p testów jednowymiarowych powoduje niekontrolowany wzrost błędu I rodzaju.\n\nTesty jednowymiarowe charakteryzują się mniejszą mocą niż testy wielowymiarowe. Zdarza się, że żaden z testów jednowymiarowych nie odrzuci hipotezy $H_0$, a test wielowymiarowy tak.\n\nTesty jednowymiarowe kompletnie ignorują korelacje pomiędzy analizowanymi chechami. (nie wuzględniają zależności analizowanych cech)\n\n<center>\n![](C:/Users/Michc/Dropbox/Uczelnia/Egzaminy/Egzaminy/obrazki/image1.png){width=50%}\n</center>\n\ntesty t-Studenta lub ANOVA można stosować osobno dla $w$ lub $h$. W pożyszym przypadku nie wykryją one istotnych różnic, ponieważ średnie są podobne dla △ i ○. Dopiero jak się spojrzy na obie cechy jednocześnie, to widać różnicę. \n\n- Testy jednowymiarowe: porównanie dwóch lub więcej grup pod względem wielkości pewnej cechy.\n\n- Testy wielowymairowe: porównanie dówych lub więcej grup pod względem wielu cech.\n\n- Kilkukrotne wykonywanie testów jednowymiarowych powoduje niekontrolowany wzrost popełnienia błędu I. rodzaju,\n\n<br/>\n\n## 2. Wymień znane Ci dwa testy wielowymiarowe.\n\n\\- $T^2$ Hotelling'a,\n\n\\- M-Shapiro test\n\n<!-- \\- Wielowymiarowy test $u=u_0$, -->\n\n\\- Test jednorodności macierzy kowariancji (test M-Box'a), \n\n\\- MANOVA: \n\n- Lambda Wilka, \n\n- Test Roy'a, \n\n- Test Pillai’a, \n\n- Test Hotelling’a-Lawley’a.\n\n<br/>\n\n## 3. Do czego służy test MANOVA i na jakiej zasadzie działa?\n\nMANOVA (Multivariante ANalysis Of VAriance)\n\n- wielowymiarowa analiza wariancji\n\n- uogólnienie testów wielowymiarowych Hotellinga.\n\n<center>\n$H_0$: &nbsp; $u_1=u_2=...=u_k$ \n\n$H_1$: &nbsp; co najmniej dwie średnie $u_j$ nie są równe\n</center>\n\nSłuży do porównywania k wektorów pod kątem średniej.\n\n<br/>\n\nPrzez analogię do jednowymiarowej analizy wariancji test opiera się na porównaniu zmienności międzygrupowej i wewnątrzgrupowej \n\n$$H=n\\sum\\limits_{i=1}\\limits^{k}(y_{i\\cdot}-y_{\\cdot\\cdot})(y_{i\\cdot}-y_{\\cdot\\cdot})'$$\n\n$$E=\\sum\\limits_{i=1}\\limits^{k}\\sum\\limits_{j=1}\\limits^{n_i}(y_{ij}-y_{i\\cdot})(y_{ij}-y_{i\\cdot})'$$\n<br/>\n\nCztery wersje MANOVA:\n\n- Lambda Wilka\n\n  $$\\Lambda = \\frac{|E|}{|E+H|}$$\n \n  Odrzucamy $H_0$, gdy $\\Lambda \\leq \\Lambda_{\\alpha,p,v_H,v_E}$.\n\n- Test Roy'a\n  \n  Poszukiwanie takiego kierunku, aby stosunek wariancji międzygrupowej do wewnątrzgrupowej był jak największy.\n  \n  $\\lambda_1$ - największa wartość własna macierzy $E^{-1}H$\n  \n  $$\\Theta = \\frac{\\lambda_1}{1+\\lambda_1}$$\n  \n  odrzucamy $H_0$, jeśli $\\Theta \\geq \\Theta_{\\alpha,s,m,N}$, \n  \n  gdzie \n  \n  $s=min(\\nu_H,p)$, \n  \n  $m = \\frac{1}{2(|\\nu_H - p|-1)}$,\n  \n  $N = \\frac{1}{2(\\nu_H - p-1)}$.\n  \n- Test Pillai’a\n  \n  Rozwinięcie testu Roy'a, opiera się na wartościach własnych $\\lambda_1,\\lambda_2,...,\\lambda_s,$ mecierzy $E^{-1}H$\n  \n  $$V^{(s)}=Tr[(E+H)^{-1}H] = \\sum\\limits_{i=1}\\limits^{s}\\frac{\\lambda_i}{1+\\lambda_i}$$\n  \n  odrzucamy $H_0$, gdy $V^{(s)}\\geq V^{(s)}_\\alpha$.\n  \n- Test Hotelling’a-Lawley’a\n  \n  $$U^{(s)}=Tr(E^{-1}H) = \\sum\\limits_{i=1}\\limits^{s}\\lambda_i$$\n  \n  odrzucamy $H_0$, jeśli $\\frac{\\nu_E}{\\nu_H}U^{(s)}$ przekraczaja wartosci krytyczne z tabeli Hotelling’a-Lawley’a.\n\n<br/>\n\n## 4. Wymień różnice pomiędzy regresją wieloraką a analizą kanoniczną.\n\nAnaliza kanoniczna jest naturalnym uogólnikiem modelu regresji wielorakiej.\n\nPolega na badaniu zależności pomiędzy dwoma zbiorami zmiennych $X \\in \\mathbb{R}^q$ oraz $Y \\in \\mathbb{R}^p$\n\nAnaliza kanoniczna ma na celu odnalezienie struktury zależnosci pomiedzy zmiennymi obu zbiorów.\n\n<!-- Czemu zatem nie wykorzystamy analizy regresji wielorakiej dla kazdej -->\n<!-- zmiennej $Yi \\in Y$ ? -->\n\n<!-- - Analiza regresji nie uwzgledniałaby wówczas struktury zależnosci pomiędzy -->\n<!-- zmiennymi zbioru $Y$ . -->\n\n<!-- -  Zakładanie natomiast, że zmienne ze zbioru $Y$ nie wykazuja miedzy soba -->\n<!-- zaleznosci jest sporym nadużyciem, poniewaz w praktyce często taka -->\n<!-- zależnosc wystepuje. -->\n\n- Regresja wieloraka jest skierowana (X objaśnia Y, ale Y w żadnym stopniu nie opisuje X).\n\n- Regresja wieloraka ignoruje strukturę zależności zmiennych objaśnianych Y.\n\n<!-- <center> -->\n<!-- ![](obrazki/Timage2.png){width=50%} -->\n<!-- </center> -->\n\n<br/>\n\n## 5. Opisz zasadę działania analizy kanonicznej.\n\n$$\\begin{pmatrix}X \\\\ Y\\end{pmatrix} \\sim \\left(\\begin{pmatrix}\\mu \\\\ \\nu\\end{pmatrix}, \\begin{pmatrix}\\Sigma_{XX}, & \\Sigma_{XY}\\\\ \\Sigma_{YX}, & \\Sigma_{YY}\\end{pmatrix}\\right)$$\n\ngdzie\n\n<center>\n$Cov(X) = \\Sigma_{XX}$ o wymiarze $q \\times q$\n\n$Cov(Y) = \\Sigma_{YY}$ o wymiarze $p \\times p$\n\n$Cov(X,Y) = E(X-\\mu)(Y-\\nu)'=\\Sigma_{XY}=\\Sigma_{YX}' \\quad [q \\times p]$\n</center>\n\n<br/>\n\n1. Analiza kanoniczna zmierza do zidentyfikowania struktury zależności pomiędzy zbiorami zmiennych $X$ i $Y$.\n\n2. Realizuje się to poprzez znalezienie pary wektorów maksymalizujących korelację kanoniczną między $X$ i $Y$.\n\n3. Na podstawie rozkładu SVD znajdujemy wektory własne macierzy $\\Sigma_{XX}^{-\\frac{1}{2}}\\Sigma_{XY}\\Sigma_{YY}^{-1}\\Sigma_{YX}\\Sigma_{XX}^{-\\frac{1}{2}}$ i $\\Sigma_{YY}^{-\\frac{1}{2}}\\Sigma_{YX}\\Sigma_{XX}^{-1}\\Sigma_{XY}\\Sigma_{YY}^{-\\frac{1}{2}}$, które definiują wektory $a$ i $b$.\n\n4. Pierwiastki niezerowych wartości własnych wspomnianych macierzy ustawione w ciągu malejącym stanowią korelacje kanoniczne kolejnych par zmiennych kanonicznych.\n\n5. Z rozkładu SVD wynika, że kolejne pary zmiennych kanonicznych są nieskorelowane ze zmiennymi kanonicznymi innych par.\n\n6. Korelacje kanoniczne są niezmiennicze ze wzgledu na przekształcenia liniowe.\n\n<!-- Analiza kanoniczna ma na celu ujawnienie struktury zależności zachodzących pomiędzy dwoma zbiorami zmiennych. -->\n\n<!-- W tym celu należy znaleźć taką parę wektorów, która będzie maksymalizować współczynnik korelacji liniowej pomiędzy kombinacjami liniowymi $a'X$ i $b'Y$. -->\n\n<!-- Celem jest więc maksymalizacja $\\rho(a'X,b'Y) = \\rho(U_1,V_1)$. -->\n\n<!-- Współczynnik korelacji osiąga maksimum dla pierwszej pary kanonicznej (dla wektorów $(a_1, b_1)$) postaci -->\n\n<!-- $$\\begin{cases}  -->\n<!-- U_1 = a'_1X=e'_1\\sum^{-\\frac{1}{2}}_{XX}X \\\\ -->\n<!-- V_1 = b'_1Y = f'_1\\sum^{-\\frac{1}{2}}_{YY}Y -->\n<!-- \\end{cases} -->\n<!-- $$ -->\n<!-- A k-tą parę kanoniczną tworzą -->\n\n<!-- $$\\begin{cases}  -->\n<!-- U_k = a'_kX=e'_k\\sum^{-\\frac{1}{2}}_{XX}X \\\\ -->\n<!-- V_k = b'_kY = f'_k\\sum^{-\\frac{1}{2}}_{YY}Y -->\n<!-- \\end{cases} -->\n<!-- $$ -->\n\n<!-- gdzie $\\lambda_i$ jest i-tą z kolei $(\\lambda_1\\geq\\ldots\\geq\\lambda_p)$ wartością własną $\\Sigma_{XX}^{-\\frac{1}{2}}\\Sigma_{XY}\\Sigma_{YY}^{-1}\\Sigma_{YX}\\Sigma_{XX}^{-\\frac{1}{2}}$, a $e_1,\\dots,e_p$ są odpowiadającym im wektorami własnymi $\\rho(U_k,V_k) = \\sqrt{\\lambda_k}$ -->\n\n<center>\n![](C:/Users/Michc/Dropbox/Uczelnia/Egzaminy/Egzaminy/obrazki/image3.png){width=70%}\n</center>\n\n<br/>\n\n## 6. Czym charakteryzują się kolejne pary zmiennych kanonicznych?\n\nKolejne pary kanoniczne są ze sobą coraz słabiej skorelowane.\nWartość korelacji k-tej pary kanonicznej wynosi\n$$\\rho(U_k,V_k) = \\sqrt{\\lambda_k}$$\n\nPonad to:\n\n- Z rozkładu SVD wynika, że kolejne pary zmiennych kanonicznych są nieskorelowane ze zmiennymi kanonicznymi innych par.\n\n- Pierwiastki niezerowych wartości własnych macierzy $\\Sigma_{XX}^{-\\frac{1}{2}}\\Sigma_{XY}\\Sigma_{YY}^{-1}\\Sigma_{YX}\\Sigma_{XX}^{-\\frac{1}{2}}$ i $\\Sigma_{YY}^{-\\frac{1}{2}}\\Sigma_{YX}\\Sigma_{XX}^{-1}\\Sigma_{XY}\\Sigma_{YY}^{-\\frac{1}{2}}$, ustawione w ciągu malejącym stanowią korelacje kanoniczne kolejnych par zmiennych kanonicznych.\n\n- Korelacje kanoniczne są niezmiennicze ze względu na przekształcenia liniowe.\n\n<br/>\n\n## 7. Jak testujemy istotność statystyczną par kanonicznych?\n\nDo badania nieskorelowania dwóch zbiorów zmiennych mozna wykorzystać test Wilka największej wiarogodności, przy założeniu normalności wielowymiarowej badanej struktury zmiennych. \nBadający hipotezę $$H_0: \\exists_k \\text{cor}(U_k,U_k) \\neq 0$$ (conajmniej jedna para kanoniczna jest istotnie skorelowana)\n\n$$\\text{O statystyce testowej }\\; T^{\\frac{2}{n}} = |I - S_{YY}^{-1}S_{YX}^{\\quad}S_{XX}^{-1}S_{XY}^{\\quad}|=\\prod\\limits_{i=1}^{k}(1-\\ell_i)$$\ngdzie $S_{YY}^{\\quad},S_{YX}^{\\quad},S_{XX}^{\\quad},S_{XY}^{\\quad}$ są odpowiednikami macierzy kowariancji $\\Sigma_{YY}^{\\quad},\\Sigma_{YX}^{\\quad},\\Sigma_{XX}^{\\quad},\\Sigma_{XY}^{\\quad}$ wyliczonymi na podstawie próby \n\noraz $\\ell_i$ jako próbkowy wskaźnik $\\lambda_i$\n\n<br/>\n\n- Test Bartletta (czyli statystyka testowa zaproksymowana do rozkładu $\\chi^2$)\n\n  Rozkład powyższej statystyki testowej jest skomplikowany, dlatego Bartlett wprowadził wzór aproksymacyjny dla dużych $n$:\n  \n  $$-\\left[n - \\frac{p+q+3}{2}\\right]log\\prod\\limits_{i=1}^{k}(1-\\ell_i)\\sim \\chi_{pq}^{2}$$\n  \n  Do testowania hipotezy, że współczynniki korelacji kanonicznych są niezerowe po usunięciu pierwszych $s$ pierwiastków (zmiennych kanonicznych) używamy statystki:\n  \n  $$-\\left[n - \\frac{p+q+3}{2}\\right]log\\prod\\limits_{i=s+1}^{k}(1-\\ell_i)\\sim \\chi_{(p-s)(q-s)}^{2}$$\n\n<br/>\n\n## 8. Co wyrażają ładunki czynnikowe w analizie kanonicznej?\n\nŁadunki czynnikowe wyrażają <u>korelację między zmiennymi kanonicznymi, a poszczególnymi zmiennymi pierwotnymi danego zbioru danych</u> (im wyższe tym silnej dana zmienna pierwotna oddziaływuje na zmienną kanoniczną).\n\n<br/>\n\n## 9. Jak określamy poziom wyjaśnionej wariancji w analizie kanonicznej?\n\nPoziom wyjaśnionej wariancji okreslamy za pomocą <u>współczynnika determinacji</u>. W CCA (*Canonical Correlation Analysis*) jest on średnią kwadratów ładunków czynnikowych, która oznacza jaki procent zmienności zbioru wyjasnia średnio dana zmienna kanoniczna w tym zbiorze danych. <br/>\nMiarą wyjaśnionej wariancji pierwotnej zmiennej przez zmienną kanoniczną jest kwadrat ładunku (korelacji).\n\n<br/>\n\n## 10. Czym jest redundancja w analizie kanonicznej?\n\n<u>Redundancja</u> - kwadrat korelacji kanonicznej pomnożony przez wariancję\nwyodrębnioną danej zmiennej kanonicznej.\n\nMówi nam o tym ile przeciętnej wariancji w jednym zbiorze jest wyjaśnione przez daną zmienną kanoniczną przy drugim zbiorze. Inaczej mówiąc dowiemy się z tego wskaźnika jak nadmiarowy jest jeden zbiór zmiennych przy takim, a nie innym składzie zmiennych w drugim zbiorze.\n\n<br/>\n\n## 11. Jakie są założenia analizy kanonicznej?\n\n- Wszystkie rozkłady zmiennych populacji z której pobieramy próbe są wielowymiarowe normalne (konsekwencje naruszenia tego założenia nie są znane).\n\n- Aby wyniki były rzetelne, zalecane jest aby liczba przypadków branych do analizy była dwudziestokrotnie większa niż liczba zmiennych.\n\n- Zmienne w obu zbiorach nie powinny być wspóliniowe.\n\n- Analiza kanoniczna jest wrażliwa na punkty odstające, które mogą zniekształcić znacząco wynik analiz.\n\n<br/>\n\n## 12. Do czego służy i jak działa analiza dyskryminacyjna?\n\nAnaliza funkcji dyskryminacyjnej stosowana jest do rozstrzygania, które zmienne pozwalają w najlepszy sposób wyróżniać (dyskryminować) dwie lub więcej wyłaniających się grup.\n\n<!-- Podstawową ideą analizy dyskryminacyjnej jest rozstrzyganie, czy miedzy grupami istnieje różnica ze względu na średnią pewnej zmiennej, a następnie wykorzystywanie tej zmiennej do okreslenia przynależności do grupy, przy możliwie minimalnych błędach klasyfikacji. -->\n\nDokonywane jest to przez przyglądanie się różnicom co do średniej zmiennych w podziale na grupy.\n\nNastępnie, za pomocą otrzymanych funkcji dyskryminacyjnych, określana jest przynależność danego obiektu do grupy.\n\n<!-- <br/> -->\n<!-- Celem jest utworzenie takiej kombinacji liniowej zmiennych niezależnych, która w najlepszy sposób dyskryminuje dwie lub więcej grup określonych *a priori*. Oznacza to wyznaczenie takich estymatorów współczynników $a_i$, które maksymalizują zmienność międzygrupową w stosunku do zmienności wewnątrz grupowej. -->\n\n<!-- Przykładowo mamy grupę klientów banku, kótrzy starają się o przyznanie im kredytu. Bank na podstawie danych obejmujących takie informacje o klientach jak: dochód, wartość kredytu w stosunku do wartości inswestycji, staż pracy, wiek kredytobiorcy, liczba rat kredytu i liczba osób w rodzinie, musi zaklasyfikować ich do grupy osoób, które spłacą kredyt, bądź do grupy, która tego kredytu nie spłaci. -->\n\n<!-- <br/> -->\n\n<!-- Wyróżniamy dwa etapy analizy dyskryminacyjnej: -->\n\n<!-- 1. Etap uczenia, gdzie znajdujemy reguły klasyfikacyjne w oparciu o tzw. zbiór uczący. -->\n\n<!-- 2. Etap klasyfikacji, w którym w oparciu o wyróżnione cechy dokonujemy klasyfikacji obiektów, których przynaleznośc jest nam nieznana. -->\n\n\n\n<br/>\n\n## 13. Czym są funkcje dyskryminacyjne?\n\n<u>Funkcje dyskryminacyjne</u> - kombinacje liniowe zmiennych niezależnych najlepiej separujące (dyskryminujące) obiekty różnych klas. (decydują o przynależności obiektu do jednej z grup).\n\n<!-- Vsh -->\n<!-- Są one postaci $$u_i = a'_ix$$gdzie <br/> $a$ (*wektor wag dyskryminacyjncyh*) maksymalizuje zmienność międzygrupową w sotsunku do zmienności wewnątrzgrupowej; jest rozwiązaniem równania $$(H-\\lambda E)a = 0$$ gdzie $$\\begin{cases} -->\n<!-- \\bar{x} = \\frac{1}{n}\\sum\\limits^{k}_{i=1} n_i\\bar{x}_i \\; \\leftarrow \\text{średnia ogólna z }n\\text{ elementowej próby}\\\\ -->\n<!-- \\bar{x}_i = \\frac{1}{n_i}\\sum\\limits^{n_i}_{j=1} x_{ij} \\; \\leftarrow \\text{wektor średnich grupowych}\\\\ -->\n<!-- H=n\\sum\\limits_{i=1}\\limits^{k}(\\bar{x}_{i}-\\bar{x})(\\bar{x}_{i}-\\bar{x})'\\\\ -->\n<!-- E=\\sum\\limits_{i=1}\\limits^{k}\\sum\\limits_{j=1}\\limits^{n_i}(x_{ij}-\\bar{x}_{i})(x_{ij}-\\bar{x}_{i})' -->\n<!-- \\end{cases}$$ -->\n\n<!-- $\\lambda = (\\lambda_1,\\dots,\\lambda_s), \\; s\\leq\\min(m,k-1)$ jest wektorem wartości własnych macierzy $E^{-1}H$. <br/> -->\n<!-- Takich funkcji możemy utworzyć $s = \\min(m,k-1)$ $k$*- liczba grup*, $m$*-liczba zmiennych* -->\n\n\n<!-- Va -->\nAlgebraicznie oznacza to zastąpienie wektora cech $x=(x_1,...,x_m)^T$ kombinacją (zwykle) liniową:\n\n$$u = a_1x_1+a_2x_2+...+a_mx_m$$\n\ngdzie $\\mathrm{a} = (a_1,...,a_m)^T$ będziemy nazywać wektorem wag dyskryminacyjnych i $x_i = (x_{i1},...,x_{in_i})$ oznacza wartości $i$-tej cechy w próbie $n_i$-elementowej.\n\nCelem jest utworzenie takiej kombinacji liniowej zmiennych niezależnych, która w najlepszy sposób dyskryminuje dwie lub więcej grup określonych *a priori*. Oznacza to wyznaczenie takich estymatorów współczynników $a_i$, które maksymalizują zmienność międzygrupową w stosunku do zmienności wewnątrz grupowej.\n\n<center>\n![Przykład działania funkcji dyskryminacyjnych](C:/Users/Michc/Dropbox/Uczelnia/Egzaminy/Egzaminy/obrazki/image4.png)\n</center>\n\n<br/>\n\n## 14. Jak wyznacza się wektor tworzący funkcje dyskryminacyjne?\n\n<!-- *Patrz wyznaczanie *$a$ *w* **13.**   -->\n\nNiech $x_{i1},x_{i2},\\dots,x_{in}$ będzie próbą prostą z grupy $i$, gdzie $i = 1,\\dots,k$ i niech $n_1+n_2+\\dots+n_k=n$.\n\n___\n\nZ próby tej obliczamy wektory średnich grupowych $$\\bar x_i = \\frac{1}{n_i}\\sum\\limits^{n_i}_{j=1}x_{ij}$$ oraz macierz kowariancji $$S_i = \\frac{1}{n_i-1}\\sum\\limits^{n_i}_{j=1}(x_{ij}-\\bar{x}_{i})(x_{ij}-\\bar{x}_{i})^T\\; \\text{ gdzie }\\; i = 1,\\dots,k$$ \n\n___\n\nNastępnie z całej $n$-elementowej próby uczącej obliczamy średnią ogólną $$\\bar{x} = \\frac{1}{n}\\sum\\limits^{k}_{i=1} n_i\\bar{x}_i$$ macierz zmienności międzygrupowej $$H=n\\sum\\limits_{i=1}\\limits^{k}(\\bar{x}_{i}-\\bar{x})(\\bar{x}_{i}-\\bar{x})^T$$ oraz macierz zmienności wewnątrzgrupowej $$E=\\sum\\limits^k_{i=1}(n_i-1)S_i$$\n\n___\n\nWektory wpsółczynników $w$ są wektorami własnymi odpowiadjącymi wartościom własnym $$\\lambda_1,\\lambda_2\\dots,\\lambda_s \\quad s\\leq\\min(m,k-1)$$ równania $$(H-\\lambda E)a = 0$$ \n\n___\n\nOtrzymujemy zatem $s$ kombinacji nazywanych liniowymi funkcjami dyskryminacyjnymi $$u_i=a_i^Tx, \\quad i= 1,\\dots,s$$ \n\nZmienne dyskryminacyjne są nieskorelowane, ale nie są ortogonalne. Zniekształcenie nie jest zwykle duże, stąd zwyczaj rysowania ich w prostokątnym układzie wpółrzędnych.\n\n<br/>\n\n## 15. Jak określa się względną miarę siły dyskryminacyjnej funkcji dyskryminacyjnej?\n\nWygodną miarą względnej siły dyskryminacyjnej i-tej zmiennej dyskryminacyjnej $u_i$ jest wielkość $$\\widetilde{\\lambda}_i = \\frac{\\lambda_i}{\\sum\\limits_{i=1}^{s}\\lambda_i} \\cdot 100\\%$$ interpretowana jako procent wariancji międzygrupowej przypadający na daną zmienną.\n\n<br/>\n\n## 16. Czym jest lambda Wilka w analizie dyskryminacyjnej?\n\nZauważmy, że $i$-ta zmienna dyskryminacyjna nie jest użyteczna w procesie klasyfikcaji jeśli odpowiadająca jej wartość własna nie jest istotnie różna od $0$.\n\nPojawia się więc pytanie o istotność otrzymanych wyników. Czy obserwowane w próbie zróżnicowanie występuje faktycznie w badanej populacji?\n\nNajczęściej stosuje się w tym celu <u>Lamdę Wilk’a, która jest miarą mocy dyskryminacyjnej modelu</u> $$\\Lambda = \\prod\\limits_{i=1}^s\\frac{1}{1+\\lambda_i}$$\n\ngdzie <br/>\n$0$ - *doskonkonała moc dyskryminacyjna* <br/>\n$1$ - *całkowity brak mocy*\n\n\n<br/>\n\n## 17. Czym jest cząstkowa lambda Wilka w analizie dyskryminacyjnej?\n\nJeżeli model jest istotny statystycznie, to nalezy sprawdzić, czy wszystkie zmienne dyskryminacyjne są istotne.\n\nDokładniej, czy zmienne po wskaźniku $p$ mają istotną miarę dyskryminacyjną.\n\nStosowana jest w tym celu analogiczna statystyka Wilk'a\n\n$$\\Lambda_p = \\prod\\limits_{i=p+1}^s\\frac{1}{1+\\lambda_i}$$\n\nWspomniana lambda Wilk'a ma w przybliżeniu rozkład $\\chi^2$ i dlatego w praktyce do testowania istotności modelu wykorzystuje się statystykę postaci $$\\chi^2 = -[n-\\frac{k+s}{2}-1]ln\\Lambda_p$$ mającą rozkład $\\chi^2$ o $(m-p)(k-p-1)$ stopniach swobody \n\nOddzielnym zagadnieniem jest to, które ze zmiennych pierowtych są ważne ze względu na własności dyskryminacyjne. W tym celu dla każdej zmiennej wyznacza się lambdę wilka według wzoru jak wyżej, ale bez udziału $i$-tej zmiennej $(\\Lambda_P^{(i)})$ \n\nStosunek $$\\frac{\\Lambda_p}{\\Lambda_p^{(i)}}$$ nazywany jest <u>cząstkową lambdą Wilk'a</u> gdzie $\\Lambda_p^{(i)}$ oznacza $\\Lambda_p$ bez udziału i-tej zmiennej.\n\nDo istotności poszególnych zmienncyh objaśniających stosujemy statystykę $$F = \\frac{n-k-m}{k-1}\\cdot\\frac{1-\\Lambda_p^{(i)}}{\\Lambda_p^{(i)}}$$ mającą rozkład $F$ o $n-k-m$ i $k-1$ stopniach swobody. \n\n<br/>\n\n## 18. Podaj założenia modelu analizy dyskryminacyjnej.\n\n- Cechy mają w grupach wielowymiarowy rozkład normalny.\n\n- Macierze wariancji/kowariancji są w grupach homogeniczne.\n\n- Brak korelacji miedzy średnimi i wariancjami.\n\n- <u>Brak współliniowości zmiennych</u> wykorzystywanych do dyskryminacji grup - (w innym przypadku będzie źle uwarunkowana macierz wariancji / kowariancji)\n\n- <u>Rozmiar próby - dobrze, aby przypadków było conajmniej 4-5 razy więcej niż zmiennych użytych do budowy modelu.</u> Najmniejsza liczebność próby powinna być większa od liczby cech $m$ (ew. $m-2$). Dobrze też, aby wszystkie grupy były równoliczne.\n\n- <u>Brak Wartości odstających</u> - podobnie jak inne metody jest wrażliwa na takie punkty. Zawyżają one sztucznie zmienność i wartości średnie co narusza założenia o jednorodności wariancji/kowariancji i braku korelacji średnich i wariancji.\n\n\n<br/>\n\n## 19. Do czego służy analiza składowych głównych?\n\nPCA służy do:\n\n- Redukcji liczby zmiennych bez istotnej straty zawartych w nich informacji.\n\n- Transformacji układu zmiennych w jakościowo nowy układ czynników głównych.\n\n- Ortogonalizacji przestrzeni, w której rozpatrywane są obiekty, będące przedmiotem badań.\n\n- Wykrywania ukrytych związków między zmiennymi – formułowania i weryfikacji hipotez dotyczących istnienia i charakteru prawidłowości kształtujących związki między zjawiskami.\n\n- Opisu zjawisk w kontekście nowych kategorii zdefiniowanych przez\nczynniki.\n\n\n<br/>\n\n## 20. Podaj interpretację geometryczną PCA.\n\nGeometrycznie chodzi o znalezienie takiego wektora(ów), w kierunku którego wariancja obserwacji w oryginalnej przestrzeni jest największa. Po znalezieniu takiego wektora (PC1), szukamy wektora prostopadłego do PC1, w kierunku którego wariancja jest największa. Procedurę tę prowadzimy do wyczerpania wymiaru przestrzeni, czyli jeśli $X$ jest $n\\times p$ wymiarowa, to możemy wyznaczyć $p$ składowych głównych\n\n<center>\n![](C:/Users/Michc/Dropbox/Uczelnia/Egzaminy/Egzaminy/obrazki/image5.png){width=100%}\n</center>\n\n<br/>\n\n## 21. - Jak wyznacza się kierunki składowych głównych?\n\nKierunki składowych głównych znajdujemy poprzez transformacje oryginalnych wartości macierzy $X$ przez ortogonalną macierz obrotu $A$.<br/>\nPróbkowa macierz kowariancji nowego układu współrzędnych ma postać\n$$S_z=ASA'$$\nZ dekompozycji spektralnej ($A = CDC′$) po prostej dedukcji mamy, że\n$$\\begin{align}\nA = C' & = \n\\begin{pmatrix}\na'_1 \\\\ a'_2 \\\\ \\vdots \\\\ a'_p\n\\end{pmatrix}\n\\end{align}$$\ngdzie<br/>\n$a'_i$ jest i-tym wektorem własnym próbkowej macierzy kowariancji $S$.\n\nW ten sposób otrzymujemy wartości składowych głównych $\\;z_1 = a′_1x ,\\; z_2 = a′_2x ,\\; \\dots ,\\; z_p = a'_px$\n\n<br/>\n\n## 22. Jak określa się miarę wyjaśnionej wariancji przez model PCA?\n\nMiarą wyjaśnionej zmienności wektora losowego $x$ przez $k$ pierwszych składowych głównych nazywamy wskaźnik $$\\frac{\\lambda_1+\\lambda_2+\\dots+\\lambda_k}{\\lambda_1+\\lambda_2+\\dots+\\lambda_p} \\cdot 100\\%$$\n\n___\n\n$\\lambda_p$ - wartości własne macierzy kowariancji $\\Sigma$\n\n<br/>\n\n## 23. Jakie znasz kryteria doboru liczby składowych głównych?\n\n- <b>Kryterium osypiska</b> - na osi odciętych zaznaczamy numer wartości własnej (wartości własne uprzednio uporządkowane w kolejności nierosnącej), na osi rzędnych nanosimy wielkość wartości własnej. Tak powstałe punkty łączymy liniami. Otrzymany wykres nazywamy wykresem piargowym (lub wykresem osypiska)\n\n- <b>Kryterium wyjaśnionej wariancji</b> - dobieramy tak dużą liczbę składowych głównych, aby przekroczyć powszechnie uznawany próg 80 % wyjaśnionej zmienności. (Ze wzoru z 22.)\n\n- <b>Kryterium Keisera</b> - zakłada, że skoro standaryzowane zmienne wejściowe niosły ze sobą wariancje na poziomie 1, to każda składowa, którą chcemy włączyć do modelu też powinna mieć wariancję (wartość własną) równą co najmniej 1.\n\n<br/>\n\n## 24. Na czym polega analiza czynnikowa?\n\nAnaliza czynnikowa polega na odtworzeniu macierzy kowariancji (korelacji) pierwotnych zmiennych w nowym układzie współrzędnych utworzonym przez czynniki. \n\nZakłada się w niej, że każdą zmienną obserwowalną można przedstawić jako kombinację liniową pewnej liczby nieobserwowalnych zmiennych, zwanych czynnikami, wspólnych dla całego zbioru zmiennych wejściowych, oraz jednego nieobserwowalnego czynnika swoistego dla tej zmiennej.\n\nModel analizy czynnikowej $$Z = WF + \\varepsilon$$ gdzie <br/>\n$W$ - macierz ($m \\times s$) ładunków czynnikowych (wag), <br/>\n$F$ - macierz ($s \\times m$) czynników wspólnych, <br/>\n$\\varepsilon$ - macierz ($m \\times 1$) czynników swoistych. <br/> <br/>\n$W$ znajdujemy w wyniku dekompozycji macierzy kowariancji $\\Sigma$.\n\n___\n\nŁadunki czynnikowe są współczynnikami korelacji pomiędzy daną zmienną a składowymi.\n\n<br/>\n\n## 25. Czym są zasoby zmienności wspólnej i zasoby zmienności swoistej?\n\nWariancję (zasób informacyjny) każdej zmiennej wyjściowej rozkłada się na dwa składniki: $$1=^1 V\\!ar(Z_j) = h^2_j + d^2_j = \\sum\\limits^s_{l=1}w^2_{jl} + V\\!ar(\\varepsilon_j)$$ gdzie <br/>\n$h^2_j$ - zasoby zmienności wspólnej (ang. communalities), <br/>\n$d^2_j$ - zasoby zmienności swoistej (ang. uniqueness).\n\n___\n\n$^1$na podstawie standaryzacji\n\n<br/>\n\n## 26. Opisz zasadę działania jednej z technik wyznaczania macierzy ładunków czynnikowych.\n\n<u>Metoda składowych głównych</u><br/>\nZe względu na nazwę cześto jest mylona z *PCA*, faktycznie niewiele ma z nią wspólnego. Nazwa bierze się z faktu, iż w modelu pomija się zasoby zmienności swoistej podczas estymacji ładunków. Przyjmuje się, że $S=\\hat W\\hat W'$. Do estymacji ładunków używamy dekompozycji spektralnej $$S=CDC'=CD^\\frac{1}{2}D^\\frac{1}{2}C'=(CD^\\frac{1}{2})(CD^\\frac{1}{2})'$$ $$\\text{Stąd }\\; \\hat W = CD^\\frac{1}{2}$$ Gdy naszą interpretacją jest redukcja przeztrzeni z $p$ do $m$ wymiarów przyjmuje się, że $$\\hat W = C_1D^\\frac{1}{2}_1$$ gdzie <br/> $C_1$, $D_1$ oznaczają zredukowane do pierwszych $m$ wartości własnych wersje macierzy i wektorów własnych macierzy $S$.\n\n<br/>\n\n## 27. Jakie znasz metody estymacji wstępnych oszacowań zasobów zmienności wspólnej?\n\nW ramach metody składowych głównych: $$\\hat h^2_i=\\sum\\limits^m_{j=1}\\hat\\omega^2_{ij}$$\n\nGdy estymujemy ładunki z macierzy korelacji $\\textbf{R}$:\n\n- *średnia arytmetyczna współczynników korelacji* danej zmiennej z innymi $$h^2_j=\\frac{1}{m}\\sum\\limits^m_{j'=1}r_{jj'} \\quad j\\neq j'$$\n\n- *maksymalna wartość bezwzględna współczynników korelacji* danej zmiennej z innymi zmiennymi $$h^2_j=\\max\\limits_{j'}|r_{jj'}| \\quad j\\neq j'$$\n\n- *współczynnik determinacji wielokrotnej* danej zmiennej z innymi zmiennymi (najczęściej stosowana i wykorzystywana przez `R`) $$h^2_j=R^2_{j\\cdot 1,2,\\dots,m}$$\n\n- *formuła triad* $$h^2_j=\\frac{r_{jj'}r_{jj''}}{r_{j'j''}} \\quad j\\neq j' \\neq j'' $$ gdzie <br/> $r_{jj'}$, $r_{jj''}$ - dwie najwyższe wartości współczynników korelacji $j$-tej zmiennej z innymi zmiennymi\n\n<br/>\n\n## 28. Na czym polega przypadek Heywood’a?\n\nZdarza się, że zasoby zmienności wspólnej przekraczają 1, co nazywamy przypadkiem Heywood’a, może on wystapić w *Metodzie iterowanych zasobów zmienności wspólnej (MINRES)* i w *Metodzie największej wiarogodności*\n\n<br/>\n\n## 29. Jakie znasz kryteria doboru liczby czynników?\n\n- Wybierz taką liczbę czynników, aby łączny poziom wyjaśnionej wariancji przekroczył $80\\%$,\n\n- Wybierz tyle czynników, ile wartości własnych jest wiekszych niż średnia wartość własna,\n\n- Użyj kryterium osypiska (opisane przy okazji PCA).\n\n- Wyznacz liczbę potrzebnych czynników na podstawie testu, który mówi, że m jest wystarczającą liczbą czynników, aby spełniona była hipoteza $H_0$: $S = \\hat{W}\\hat{W}'+\\hat{\\Psi}$.\n\n- Użyj <u>*kryterium resztowego*</u> - kryterium to opiera się na macierzy resztowej $R − \\tilde R$, która jest różnicą macierzy korelacji i zredukowanej macierzy korelacji. Jest ona miarą dopasowania modelu zawierającego odpowiednią liczbę czynników do danych obserwowanych. Przyjmujemy taką liczbę czynników, począwszy od której odchylenie standardowe wyrazów macierzy powyżej głównej przekątnej jest mniejsze od $\\frac{1}{\\sqrt{n-2}}$\n\n<br/>\n\n## 30. Na czym polega rotacja układu w analizie czynnikowej?\n\nJest to metoda obracania układu współrzędnych, w taki sposób, aby umożliwić badaczowi łatwiejszą interpretację czynników. Transformacje te powinny prowadzić do prostych wyników. Wyróżnia się rotację ortogonalną i nieortogonalną (ukośną).\nDążymy do tego aby ładunki czynnikowe miały wartości jak najbliższe 0 lub najbardziej skrajne, czyli bliskie -1 albo 1\n\nUdział czynników w wyjaśnianiu wspólnej wariancji (suma ich zasobów informacyjnych) nie ulega zmianie w wyniku rotacji)\n\nRotacje prowadzą do wyodrębnienia rozłącznych grup zmiennych wejściowych, z których każda zawiera zmienne o wysokich ładunkach dla jednego czynnika, średnie dla innych czynnikówo raz bliskie zeru dla pozostałych czynników.\n\n<br/>\n\n## 31. Wymień po jednej rotacji ortogonalnej i ukośnej.\n\nRotacje ortogonalne\n\n- <u>*Metoda Varimax*</u> - Metoda ta pozwala na minimalizację liczby zmiennych posiadających wysokie ładunki czynnikowe przez obrót ortogonalny. Upraszcza w ten sposób interpretację czynników.\n\n- <u>*Metoda Quartimax*</u> - Metoda rotacji, która minimalizuje liczbę czynników potrzebnych do wyjaśnienia każdej zmiennej. Metoda ta upraszcza interpretację obserwowanych zmiennych.\n\nRotacje ukośne \n\n- <u>*Metoda rotacji prostą OBLIMIN*</u> - Metoda ta pozwala wyodrębnić ładunki czynnikowe przez obrót ukośny (dla czynników skorelowanych ze sobą)\n\n- <u>*Metoda Promax*</u> - Metoda która pozwala na skorelowanie czynników. Można ją wyliczyć szybciej niż rotację prostą Oblimin, dlatego jest ona użyteczna w przypadku dużych zbiorów danych.\n\n<br/>\n\n## 32. Na czym polega analiza skupień?\n\nGrupowanie (ang. data clustering), zwane również analizą skupień lub klasyfikacją nienadzorowaną polega na podziale pewnego zbioru danych $$O = \\{x_i = (x_{i1},\\dots,x_{id}),\\; i=1,\\dots,N\\}$$ na pewne podzbiory wektorów (grupy).\n\nPodstawowym założeniem dotyczącym wynikowego podziału jest homogeniczność obiektów wchodzących w skład jednej grupy oraz heterogeniczność samych grup – oznacza to, że wektory stanowiące jedną grupę powinny być bardziej podobne do siebie niż do wektorów pochodzących z pozostałych grup.\n\n___\n\n$x_i$ jest $d$-wymiarowym wektorem cech opisujących obiekt należący do zbioru.\n\n<br/>\n\n## 33. Jakie warunki spełnia podział twardy?\n\nPodział twardy (ang. hard) uzyskuje się w efekcie takiego grupowania, w którym każdy wektor (obiekt) należy dokładnie do jednej grupy i wszystkie grupy są niepuste.\n\nIstnieją również metody analizy skupień (ang. *fuzzy clustering*) oparte o grupowanie probabilistyczne, w którym obiekty należą z pewnym prawdopodobieństwem (nie zakałda się jednoznaczności przypisania).\n\n<br/>\n\n## 34. Czym się różnią grupowania hierarchiczne od niehierarchicznych?\n\nCelem algorytmów <u>niehierarchicznych</u> jest znalezienie takiego podziału zbioru na zadaną liczbę podzbiorów, aby uzyskać optymalną wartość pewnego kryterium. Optymalizację kryterium osiąga się np. poprzez iteracyjne przemieszczanie obiektów między grupami.\n\n<u>Metody hierarchiczne</u> konstruują pewną hierarchię skupień, która najczęściej reprezentowana jest graficznie w postaci drzewa binarnego nazywanego dendrogramem. W liściach takiego drzewa znajdują się elementy analizowanego zbioru, węzły natomiast stanowią ich grupy\n\n<br/>\n\n## 35. Opisz algorytm grupowania metodą k-średnich.\n\n1. Podziel wstępnie zbiór na k skupień (losowo),\n\n2. Dla każdego skupienia policz jego centroid (środek ciężkości grupy),\n\n3. Przypisz każdy z elementów zbioru do najbliższej mu grupy (odległość od grupy jest w tym przypadku tożsama z odległością od centroidu),\n\n4. Powtarzaj dwa poprzednie kroki tak długo, jak długo zmienia się przyporządkowanie obiektów do skupień.\n\nNiestety algorytm k-średnich ma wiele wad. Już na wstępie konieczne jest zdefiniowanie liczby grup, chociaż zazwyczaj nie wiadomo, jak wiele grup występuje w przetwarzanym zbiorze. Początkowe centroidy wybierane są w sposób losowy, podczas gdy ich wybór ma decydujący wpływ na jakość otrzymanego grupowania. Ponadto algorytm jest mało odporny na zaszumione dane.\n\n<br/>\n\n## 36. Czym są metody aglomeracyjne i deglomeracyjne?\n\n<u>Metody aglomeracyjne</u> rozpoczynają tworzenie hierarchii od podziału zbioru n obserwacji na n jednoelementowych grup, które w kolejnych krokach są ze sobą scalane.\n\n<u>Metody deglomeracyjne</u> inicjowane są jedną grupą n-elementową, a hierarchia tworzona jest poprzez sukcesywny podział na coraz mniejsze grupy\n\n\n<br/>\n\n## 37. Wymień co najmniej trzy metryki stosowane w analizie klastrowej.\n\n- Odległość euklidesowa $$d(x,y) = \\sqrt{\\sum\\limits^k_{i=1}(x_i-y_i)^2}$$\n\n- Kwadrat odległości euklidesowej $$d^2(x,y) = \\sum\\limits^k_{i=1}(x_i-y_i)^2$$\n\n- Odległość Manhattan (taxi, miejska) $$d(x,y) = \\sum\\limits^k_{i=1}|x_i-y_i|$$\n\n- Odległość Czebyszewa $$d(x,y) = \\max\\limits_{i=1,\\dots,k}|x_i-y_i|$$\n\n<br/>\n\n## 38. Opisz co najmniej trzy sposoby aglomeracji.\n\n- <u>Pojedyńczego wiązania</u> $$d(A,B) = \\min \\{d(x,y): x\\in A, y \\in B\\}$$\n\n- <u>Pełnego wiązania</u> $$d(A,B) = \\max \\{d(x,y): x\\in A, y \\in B\\}$$\n\n- <u>Środków ciężkości</u> - środek ciężkości skupienia jest to punkt o współrzędnych będących średnimi arytmetycznymi wartości zmiennych dla obiektów należących do danego skupienia; odległość skupień jest definiowana jako odległość ich środków ciężkości\n\n- <u>Ważonych środków ciężkości</u> – analogicznie jak poprzednio z tym, że przy obliczeniach uwzględnia się ważenie, aby uwzględnić różnice w liczebnościach skupień,\n\n- <u>Warda</u> -  ta metoda różni się od wszystkich pozostałych, ponieważ do oszacowania odległości między skupieniami wykorzystuje podejście analizy wariancji. Zmierza do minimalizacji sumy kwadratów odchyleń dowolnych dwóch skupień, które mogą zostać uformowane na każdym etapie.\n\n\n<br/>\n\n## 39. Jak przebiega algorytm grupowania hierarchicznego?\n\n1. Wyznaczenie macierzy odległości pomiędzy obiektami;\n\n2. Wybór najmniejszej odległości (poza przekątną) – tzw. odległości aglomeracyjnej;\n\n3. Połączenie odpowiadających jej obiektów;\n\n4. Wyznaczenie nowej macierzy odległości;\n\n5. Wybór nowej odległości aglomeracyjnej;\n\n6. Połączenie odpowiadających jej obiektów lub skupień;\n\n7. Powrót do punktu 4 aż do połączenia wszystkich obiektów w jedno skupienie.\n\n<br/>\n\n## 40. Do czego służy analiza korespondencji?\n\n<!-- Analiza korespondencji przeznaczona jest do analizy zmiennych o charakterze jakościowym, tj. mierzonych na słabych skalach pomiaru (nominalna, porządkowa, przedziałowa). Metoda ta pozwala na graficzne przedstawienie wyników analizy danych w postaci mapy percepcji w niskowymiarowej przestrzeni. -->\n\n<!-- Analiza korespondencji jest metodą badania współwystępowania zmiennych, umożliwiającą graficzną prezentację wyników w postaci mapy percepcji. Jej głównym celem jest prezentacja wyników w postaci mapy percepcji, na której przedstawione są wszystkie kategorie badanych zmiennych. -->\n\nAnaliza korespondencji jest metodą badania współwystępowania zmiennych. Przeznaczona jest do analizy zmiennych o charakterze jakościowym, tj. mierzonych na słabych skalach pomiaru (nominalna, porządkowa, przedziałowa). Metoda ta pozwala na graficzne przedstawienie wyników analizy w postaci mapy percepcji, w niskowymiarowej przestrzeni, na której przedstawione są wszystkie kategorie badanych zmiennych.\n\nSkłada się ona z 3 podstawowych kroków:\n\n- obliczania mas wierszy/kolumn\n\n- obliczania profili wierszy/kolumn\n\n- wyznaczania odległości pomiedzy wierszami lub kolumnami za pomocą statystyki $\\chi^2$\n\n<br/>\n\n## 41. Czym jest macierz kontyngencji?\n\nTabela kontyngencji prezentuje strukturę danych o charakterze jakościowym i jest punktem wyjścia do pomiaru siły zależności między dwiema zmiennymi.\n\n![](C:/Users/Michc/Dropbox/Uczelnia/Egzaminy/Egzaminy/obrazki/image6_2.jpg)\n\nEmpiryczne liczebności w $h$-tym wierszu i $j$-tej kolumnie oznaczone są przez $n_{hj}$ i oznaczają liczbę jednoczesnych wystąpień $h$-tej kategorii cechy $X$ i $j$-tej kategorii cechy $Y$. Liczebności brzegowe wierszy to liczba wszystkich wsytąpień cechy X na pewnym poziomie $$n_{h\\cdot}=\\sum\\limits^J_{j=1}n_{hj}$$ a liczebności brzegowe kolumn to: $$n_{\\cdot j}=\\sum\\limits^H_{h=1}n_{hj}$$ Tablica kontyngencji jest podstawą do zbudowania tablicy korespondencji P.\n\n<br/>\n\n## 42. Czym jest macierz korespondencji?\n\nTablica korespondencji jest wyznaczana na podstawie macierzy korespondencji i wyraża względną częstość wystąpień. Zdefinniowana jest jako $$P= \\left[\\frac{n_{hj}}{n}\\right]$$\n\n![](C:/Users/Michc/Dropbox/Uczelnia/Egzaminy/Egzaminy/obrazki/image7_2.jpg)\n\n<br/>\n\n## 43. Wymień po dwie miary zależności dla skal nominalnej i porządkowej.\n\n<u>**Miary zależności dla skal nominalnych**</u>\n\n- <u>Test $\\chi^2$</u> - sprawdzaniem hipotezy o niezależności jest statystyka: $$\\chi^2=\\sum\\limits^H_{h=1}\\sum\\limits^J_{j=1}\\frac{(n_{hj}-\\hat n_{hj})^2}{\\hat n_{hj}}$$ gdzie <br/> $\\hat n_{hj} = \\frac{n_{h\\cdot}n_{\\cdot j}}{n}$ oznaczją teoretyczne liczebności tablicy kontyngencji. <br/> $0\\neq\\chi^2\\neq n\\sqrt{(H-1)(J-1)}$ i im bliżej 0, tym bardziej prawdopodobne, że $X$ i $Y$ są niezalezne.\n\nStatystyka ma rozkład $\\chi^2$ o $(H-1)(J-1)$ stopniach swobody. Zbiór krytyczny $W$ okreslony jest relacją $P(\\chi^2\\geq\\chi^2_\\alpha)=\\alpha$\n\n- <u>$\\Phi$ Yule'a</u> $$\\Phi^2=\\frac{\\chi^2}{n}$$ \n\nmiara z przedziału $[0,1)$; interpretacja podobna do $\\chi^2$\n\n<u>**Miary zależności dla skal porządkowych**</u>\n\n- <u>$\\tau_\\alpha$ Kendalla</u> $$\\tau_\\alpha = \\frac{n_Z-n_N}{\\frac{1}{2}n(n-1)}$$ gdzie <br/>\n$n_Z$ - liczba par zgodnych tzn. porównywane zmienne w obrębie tych dwóch obserwacji zmieniają się w tę samą stronę, czyli albo w pierwszej obserwacji obydwie są więkze niż w drugiej, albo obydwie mniejsze, <br/>\n$n_N$ - liczba par niezgodnych, tzn. zmieniają się w przeciwną stronę, czyli jedna z nich jest większa dla tej obserwacji w parze, dla której druga jest mniejsza. Współczynnik Kendalla służy do oceny siły związku między zmiennymi.\n\nPrzyjmuje wartości w przedziale $[-1,1]$, przy czym wartości bliskie $1$ oznaczają, że każda ze zmiennych rośnie przy wzroście drugiej, natomiast $-1$ oznacza, że każda zmienna maleje przy wzroście drugiej.\n\n- <u>$\\gamma$ Goodmana-Kruskala</u> $$\\gamma=\\frac{n_Z-n_N}{n_Z+n_N}$$\n\nSłuży on do oceny kierunku i syły związku. Przyjmuje wartości z przedziału $[-1,1]$, przy czym wartości bliskie $-1\\;$i$\\;1$ oznaczają silną zależność, a bliskie $0$ brak zależności pomiędzy zmiennymi.\n\n\n<br/>\n\n## 44. Czym są masy wierszowe i kolumnowe?\n\nCzęstości brzegowe <br/> \nwierszy $$p_{h\\cdot}=\\sum\\limits^J_{j=1}p_{hj}=\\frac{n_{h\\cdot}}{n}$$\nkolumn $$p_{\\cdot j}=\\sum\\limits^H_{h=1}p_{hj}=\\frac{n_{\\cdot h}}{n}$$\ni nazywane są <u>*masami wierszy*</u> i <u>*masami kolumn*</u>\n\nElementy $p_{h\\cdot}$ i $p_{\\cdot j}$ tworzą odpowienio wektory częstości brzegowych wierszy $\\textbf{r}$ i wektory częstości brzegowych kolumn $\\textbf{c}$ $$\\textbf{r} = \\left[ \\frac{n_{h \\cdot}}{n} \\right] = [p_{h\\cdot}], \\quad \\textbf{c} = \\left[ \\frac{n_{\\cdot j}}{n} \\right] = [p_{\\cdot j}]$$\n\n<br/>\n\n## 45. Czym są przeciętne profile wierszowe i kolumnowe?\n\nMasy wierszowe i kolumnowe można traktować odpowiednio jako przeciętne profile kolumnowe i wierszowe ($\\leftrightarrows$).\n\nProfile wierszy i kolumn mogą być interpretowane jako punkty w wielowymiarowej przestrzeni. Profile podobne do siebie będą położone bliżej siebie,natomiast niepodobne będą przedstawione jako punkty leżące daleko od siebie.\n\n<br/>\n\n## 46. Jak obliczyć odległość pomiędzy profilami?\n\nOdległość pomiędzy profilami wierszowymi obliczamy według wzoru $$d(h,h') = \\sqrt{\\sum\\limits^J_{j=1}\\frac{\\left(\\frac{p_{hj}}{p_{h\\cdot}}-\\frac{p_{h'j}}{p_{h'\\cdot}}\\right)^2}{p_{\\cdot j}}}$$ gdzie $h,h' = 1,\\dots,H,h \\neq h'$, oznaczają dwie rózne kategorie zmiennej wierszowej.\n\nOdległość pomiędzy profilami kolumnowymi obliczamy według wzoru $$d(j,j') = \\sqrt{\\sum\\limits^H_{h=1}\\frac{\\left(\\frac{p_{hj}}{p_{\\cdot j}}-\\frac{p_{hj'}}{p_{\\cdot j'}}\\right)^2}{p_{h\\cdot}}}$$ gdzie $j,j' = 1,\\dots,J,j \\neq j'$, oznaczają dwie rózne kategorie zmiennej kolumnowej.\n\n<br/>\n\n## 47. Czym jest inercja w analizie korespondencji?\n\n<u>Inercja(bezwładność)</u> - jest miarą zróżnicowania elementów w macierzy danych wejściowych, natomiast całkowita inercja określa stopień dyspersji profili wierszowych (kolumnowych) względem odpowiadających im centroid i wskazuje, jak bardzo dane profile różnią się od odpowiadającego im profilu przeciętnego.\n\n<br/>\n\n## 48. Wymień miary jakości odtworzenia informacji w mapie percepcji.\n\n- <u>korelacja punktu z osią</u> - dzięki niej możliwe jest wskazanie tej osi, która najlepiej opisuje punkt (kategorię) w nowej przestrzeni $$\\text{cor}^2_{hk} = \\frac{p_{h\\cdot}f^2_{hk}}{\\sum\\limits^K_{k=1}p_{h\\cdot}f^2_{hk}} = \\frac{\\lambda_{hk}}{\\lambda_{h}}$$\n$$\\text{cor}^2_{jk} = \\frac{p_{\\cdot j}g^2_{jk}}{\\sum\\limits^K_{k=1}p_{\\cdot j}f^2_{jk}} = \\frac{\\lambda_{jk}}{\\lambda_{j}}$$ gdzie <br/> $f_{hk}$ - współrzędna $h$-tego wiersza w $k$-tym wymiarze <br/> $g_{jk}$ - współrzędna $j$-tej kolumny w $k$-tym wymiarze <br/> $\\lambda_{hk}$ - inercja $h$-tego wiersza w $k$-tym wymiarze <br/> $\\lambda_{h}$ - inercja $h$-tego wiersza <br/> $\\lambda_{jk}$ - inercja $j$-tej kolumny w $k$-tym wymiarze <br/> $\\lambda_{h}$ - inercja $j$-tej kolumny\n\n- <u>udział punktu w wymiarze</u> - inaczej absolutny udział, jest interpretowany jako części inercji związana z konkretnym wymiarem, która jest wyjaśniana przez dany punkt i obrazuje, do jakiego stopnia punkt przyczynia się do zdefiniowania danego wymiaru. Punkty z relatywnie wysokimi wartościami absolutnego udziału są najważniejsze w definiowaniu danego wymiaru. Suma udziałów absolutnych dla każdego wymiaru wynosi 1. Udział wiersza w wymiarze określa relacja: $$q_{hk}=\\frac{r_hf^2_{hk}}{\\lambda^2_k}$$ gdzie <br/> $r_h$ - masa $h$-tego wiersza, <br/> $\\lambda^2_k$ - wartość własna $k$-tego wymiaru.\n\nUdział kolumn określamy analogicznie: $$q_{jk}=\\frac{c_jg^2_{jk}}{\\lambda^2_k}$$ gdzie <br/> $c_j$ - masa $h$-tego wiersza, <br/> $\\lambda^2_k$ - wartość własna $k$-tego wymiaru.\n\n- <u>udział wymiaru w inercji</u> - stosunek kwadratu odległości danego punktu w tym wymiarze od środka układu osi czynnikowych do odległości od środka układu czynnikowego: $$s_h=\\frac{f^2_{hk}}{d^2_h}=\\cos^2\\!\\omega$$ gdzie <br/> $f^2_{hk}$ - - współrzędna punktu $h$ na $k$-tej osi, <br/> \n$d^2_h$ - odległość pomiędzy $h$-tym punktem a centroidą, <br/> $\\omega$ - kąt pomiędzy osią a odcinkiem łączącym punkt z\ncentroidą.\n\nPodobnie definiuje się udział kolumn: $$s_j=\\frac{g^2_{jk}}{d^2_h}=\\cos^2\\!\\omega$$ Jeśli wartość $s_h$ lub $s_j$ jest wysoka, to kąt jest mały i oznacza to, że wymiar\ndobrze opisuje ten punkt. Suma wartośći $s$ dla każdego punktu wynosi 1.\n\n<br/>\n\n## 49. Czym jest analiza log-liniowa i do czego służy?\n\nAnaliza log-liniowa jest analizą tabel wielodzielczych (tabel kontyngencji), służąca do badania wpływu różnych czynników i ich interakcji.\n\nAnaliza log-liniowa jest modelem regresji dla zmiennych jakościowych.\n\nModele log-liniowe są podobne do analizy wariancji i wiele terminów używanych w analizie wariancji zostało zaadoptowanych przez analizę log-liniowa.\n\n<br/>\n\n## 50. Czym są zera próbkowe i strukturalne w tablicach kontyngencji?\n\nProblem mogą stwarzać przypadki dopasowywania modelu log-liniowego do danych z tablic wielodzielczych, w których występują zerowe liczebności, ponieważ funkcja logarytm nie jest określona w zerze, a prawostronna granica w tym punkcie wynosi $−\\infty$.\n\nTakie sytuacje moga wystapic w dwóch przypadkach: <br/>\n- gdy nie jest możliwe zaobserwowanie wartości dla pewnych kombinacji poziomów zmiennych - <u>zera *a priori* (strukturalne)</u>; <br/>\n- gdy obserwacje są zróżnicowane, komórek jest dużo, a liczebność próby mała - <u>zera próbkowe</u>.\n\nRozwiązaniem problemu występowania zer próbkowych jest zwiększenie liczebności próbki lub ewentualnie, jeśli jest to niemożliwe, zwiększenie wszystkich liczebności oczekiwanych przez dodanie małej stałej, zwykle $\\Delta = 0.5$.\n\nW przypadku niekompletnych tablic z zerami strukturalnymi liczbę stopni swobody rozkładu $\\chi^2$ statystyki $\\chi^2$ (lub $\\chi^2_L$) określa formuła $$d\\!f=n_1-n_2-n_3$$ gdzie <br/>\n- $n_1$ - liczba komórek w tabeli, <br/>\n- $n_2$ - liczba parametrów w modely wymagających estymacji, <br/>\n- $n_3$ - liczba zer *a priori*\n\n<br/>\n\n## 51. Czym są modele hierarchiczne w analizie log-liniowej?\n\nModel hierarchiczny zawiera wszystkie składniki niższego rzędu. Jeżeli na przykład model zawiera $\\lambda^{XX}_{ij}$, to zawiera również $\\lambda^X_i$ i $\\lambda^Y_j$.\n\nJeżeli nie włączymy składników niższego rzędu, to istotność statystyczna i interpretacja składników wyższego rzędu będzie zależała od kodowania zmiennych, co jest niepożądane.\n\nJeśli model zawiera składniki dwuczynnikowe, to należy być ostrożnym w interpretacji składników niższego rzędu, gdyż szacowanie efektów głównych zależy od sposobu kodowania zmiennych zastosowanego do efektów wyższego rzędu. Prawidłowo wiec powinniśmy ograniczyć uwagę do interpretacji składników najwyższego rzędu.\n\n<br/>\n\n## 52. Do czego służy skalowanie wielowymiarowe?\n\nSkalowanie wielowymiarowe (ang. Multidimensional Scaling) jest zbiorem technik opartych na założeniu, że respondent wyrażający swój stosunek do rzeczywistości operuje w sposób mniej lub bardziej świadomy wymiarami, traktując obiekty jako punkty w przestrzeni m-wymiarowej.\n\nPodstawowe cele skalowania wielowymiarowego to:\n\n- przedstawienie w przestrzeni $r$-wymiarowej $(r < m)$ relacji zachodzących między badanymi obiektami;\n\n- ukazanie “struktury” badanych obiektów przez określenie treści wymiarów na podstawie podobieństw i preferencji respondentów;\n\n- wykrycie ukrytych zmiennych, które choć nie są obserwowane bezpośrednio, wyjaśniają podobieństwa i różnice pomiędzy badanymi obiektami;\n\n- weryfikacja hipotezy o tym, że pomiędzy badanymi obiektami faktycznie zachodzą (lub nie zachodzą) określone różnice.\n\nDecyzja o liczbie wymiarów $r$, w których prezentowane są wyniki skalowania wielowymiarowego, należy do badacza i zależy od tego, ile wymiarów powinna mieć przestrzeń stanowiąca zadowalające rozwiązanie w odniesieniu do danych wyjściowych. Ze względu na możliwości graficznej prezentacji wyników jest to zazwyczaj przestrzeń dwu- lub trójwymiarowa.\n\n<br/>\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"embed-resources":true,"self-contained":true,"output-file":"WAD_egzamin.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.475","theme":{"light":"pulse","dark":"darkly"},"title":"Wielowymiarowa analiza danych","author":"Michał Koziński","date":"02-10-2023","page-layout":"full"},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"WAD_egzamin.pdf"},"language":{},"metadata":{"block-headings":true,"documentclass":"scrreprt","title":"Wielowymiarowa analiza danych","author":"Michał Koziński","date":"02-10-2023"},"extensions":{"book":{}}}}}